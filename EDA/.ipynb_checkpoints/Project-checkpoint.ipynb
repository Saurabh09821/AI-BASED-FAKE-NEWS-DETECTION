{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a20ca9-1af5-4367-bccf-110aa9c34949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\hp\\sklearn-env\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\sklearn-env\\lib\\site-packages (from nltk>=3.9->textblob) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\sklearn-env\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\sklearn-env\\lib\\site-packages (from nltk>=3.9->textblob) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\sklearn-env\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\sklearn-env\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fa986a-5153-4d32-a96e-d4b14cdfffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Imports\n",
    "# ---------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "632b8b8b-f207-46af-8ca5-655b48ab4132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: F:/PG-DBDA-2025/Project_Upload/FAKE-NEWS-CLASSIFIER/Data_using/bharatfakenewskosh/xlsx\n",
      "Could not read as CSV. Please set DATA_PATH to your CSV file path. Error: [Errno 2] No such file or directory: 'F:/PG-DBDA-2025/Project_Upload/FAKE-NEWS-CLASSIFIER/Data_using/bharatfakenewskosh/xlsx'\n",
      "Created empty skeleton DataFrame with expected columns. Replace DATA_PATH and re-run.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Stop - set DATA_PATH to real CSV file and re-run.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Stop - set DATA_PATH to real CSV file and re-run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\sklearn-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Complete Python EDA script for your Fake News dataset\n",
    "# Replace DATA_PATH with your CSV file path if needed.\n",
    "# DEV NOTE: The path below was provided in the conversation history; replace with actual CSV path.\n",
    "DATA_PATH = \"F:/PG-DBDA-2025/Project_Upload/FAKE-NEWS-CLASSIFIER/Data_using/bharatfakenewskosh/xlsx\"  # <-- replace with \"your_dataset.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# Imports\n",
    "# ---------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "\n",
    "# If nltk resources not downloaded, uncomment:\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# ---------------------------\n",
    "# Utility functions\n",
    "# ---------------------------\n",
    "def clean_publish_date(text):\n",
    "    \"\"\"\n",
    "    Normalize publish_date values like \"9th July 2022\" to a parseable date.\n",
    "    Returns pd.Timestamp or NaT.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return pd.NaT\n",
    "    # Remove ordinal suffixes: 1st, 2nd, 3rd, 4th...\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', text, flags=re.IGNORECASE)\n",
    "    # Try parsing common formats\n",
    "    for fmt in [\"%d %B %Y\", \"%d %b %Y\", \"%Y-%m-%d\", \"%d/%m/%Y\"]:\n",
    "        try:\n",
    "            return pd.to_datetime(text, format=fmt)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Fallback - try pandas parser\n",
    "    try:\n",
    "        return pd.to_datetime(text, dayfirst=True, errors='coerce')\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def text_length_stats(series):\n",
    "    \"\"\"Return DataFrame with word count, char count stats for a text series.\"\"\"\n",
    "    s = series.fillna(\"\")\n",
    "    words = s.map(lambda x: len(str(x).split()))\n",
    "    chars = s.map(lambda x: len(str(x)))\n",
    "    return pd.Series({\n",
    "        'count': s.shape[0],\n",
    "        'non_null': s.map(bool).sum(),\n",
    "        'mean_words': words.mean(),\n",
    "        'median_words': words.median(),\n",
    "        'std_words': words.std(),\n",
    "        'mean_chars': chars.mean(),\n",
    "    })\n",
    "\n",
    "def top_n_words(series, n=30, language='english'):\n",
    "    \"\"\"Return top n words excluding stopwords.\"\"\"\n",
    "    text = \" \".join(series.dropna().astype(str).tolist()).lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stops = set(stopwords.words(language))\n",
    "    words = [w for w in tokens if w.isalpha() and w not in stops]\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "def top_ngrams(series, ngram=2, top_n=20):\n",
    "    \"\"\"Return top n n-grams from a text series.\"\"\"\n",
    "    text = \" \".join(series.dropna().astype(str).tolist()).lower()\n",
    "    tokens = [t for t in nltk.word_tokenize(text) if t.isalpha()]\n",
    "    ng = ngrams(tokens, ngram)\n",
    "    return Counter(ng).most_common(top_n)\n",
    "\n",
    "def simple_sentiment(text):\n",
    "    \"\"\"Return polarity & subjectivity using TextBlob (works on English)\"\"\"\n",
    "    if pd.isna(text) or str(text).strip()==\"\":\n",
    "        return pd.Series([np.nan, np.nan])\n",
    "    tb = TextBlob(str(text))\n",
    "    return pd.Series([tb.sentiment.polarity, tb.sentiment.subjectivity])\n",
    "\n",
    "# ---------------------------\n",
    "# Load dataset\n",
    "# ---------------------------\n",
    "print(\"Loading dataset:\", DATA_PATH)\n",
    "# attempt to load CSV; if path is image or wrong, user must replace with CSV.\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "except Exception as e:\n",
    "    print(\"Could not read as CSV. Please set DATA_PATH to your CSV file path. Error:\", e)\n",
    "    # create an empty df skeleton with columns from conversation for guidance\n",
    "    cols = [\"id\",\"Author_Name\",\"Fact_Check_Source\",\"Source_Type\",\"Statement\",\"Eng_Trans_Statement\",\n",
    "            \"News Body\",\"Eng_Trans_News_Body\",\"Media_Link\",\"Publish_Date\",\"Fact_Check_Link\",\n",
    "            \"News_Category\",\"Language\",\"Region\",\"Platform\",\"Text\",\"Video\",\"Image\",\"Label\"]\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    print(\"Created empty skeleton DataFrame with expected columns. Replace DATA_PATH and re-run.\")\n",
    "    # Stop further execution to avoid misleading outputs\n",
    "    raise SystemExit(\"Stop - set DATA_PATH to real CSV file and re-run.\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# ---------------------------\n",
    "# PHASE 1: Basic overview\n",
    "# ---------------------------\n",
    "print(\"\\n== Basic info ==\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n== Summary statistics (object cols) ==\")\n",
    "print(df.describe(include='object').T)\n",
    "\n",
    "# Missing values per column\n",
    "miss = df.isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nMissing value fraction per column:\\n\", miss)\n",
    "\n",
    "# Visual missing heatmap (save figure)\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.heatmap(df.isna(), cbar=False)\n",
    "plt.title(\"Missing data heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"missing_heatmap.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Duplicates\n",
    "dup_count = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows (full-row): {dup_count}\")\n",
    "# Check duplicates based on text fields (Statement + News Body)\n",
    "dup_text = df.duplicated(subset=[\"Statement\",\"News Body\"]).sum() if set([\"Statement\",\"News Body\"]).issubset(df.columns) else 0\n",
    "print(f\"Duplicate Statement+News Body: {dup_text}\")\n",
    "\n",
    "# ---------------------------\n",
    "# PHASE 2: Data types & conversions\n",
    "# ---------------------------\n",
    "# Lower-case columns for convenience\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Convert Label to numeric if possible\n",
    "if \"Label\" in df.columns:\n",
    "    try:\n",
    "        df[\"Label\"] = pd.to_numeric(df[\"Label\"], errors='coerce').astype('Int64')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Parse Publish_Date if present\n",
    "if \"Publish_Date\" in df.columns:\n",
    "    df[\"publish_date_parsed\"] = df[\"Publish_Date\"].apply(clean_publish_date)\n",
    "    print(\"\\nParsed publish_date sample:\")\n",
    "    print(df[[\"Publish_Date\",\"publish_date_parsed\"]].head())\n",
    "    # Extract year/month/day\n",
    "    df[\"pub_year\"] = pd.DatetimeIndex(df[\"publish_date_parsed\"]).year\n",
    "    df[\"pub_month\"] = pd.DatetimeIndex(df[\"publish_date_parsed\"]).month\n",
    "    df[\"pub_dayofweek\"] = pd.DatetimeIndex(df[\"publish_date_parsed\"]).day_name()\n",
    "    # Plot monthly/yearly distribution\n",
    "    plt.figure(figsize=(10,4))\n",
    "    df[\"pub_month\"].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title(\"Count by publish month\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"publish_month_count.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------\n",
    "# PHASE 3: Categorical EDA\n",
    "# ---------------------------\n",
    "cat_cols = [\"Author_Name\",\"Fact_Check_Source\",\"Source_Type\",\"News_Category\",\"Language\",\"Region\",\"Platform\"]\n",
    "cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "for c in cat_cols:\n",
    "    print(f\"\\n-- Top values for {c} --\")\n",
    "    print(df[c].value_counts().head(15))\n",
    "    # Save barplot for top 10\n",
    "    plt.figure(figsize=(8,4))\n",
    "    df[c].value_counts().head(10).plot(kind='barh')\n",
    "    plt.title(f\"Top 10 {c}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"top_{c}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Label distribution\n",
    "if \"Label\" in df.columns:\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    print(df[\"Label\"].value_counts(dropna=False))\n",
    "    plt.figure(figsize=(5,4))\n",
    "    df[\"Label\"].value_counts().plot(kind='bar')\n",
    "    plt.title(\"Label distribution (False/True or 0/1)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"label_distribution.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Media type counts (Text, Video, Image)\n",
    "media_cols = [c for c in [\"Text\",\"Video\",\"Image\"] if c in df.columns]\n",
    "if media_cols:\n",
    "    print(\"\\nMedia columns counts (non-null true/false):\")\n",
    "    for c in media_cols:\n",
    "        # normalize common boolean-like values\n",
    "        vc = df[c].fillna(\"no\").astype(str).str.lower().value_counts()\n",
    "        print(f\"{c}:\\n{vc}\")\n",
    "    # plot\n",
    "    plt.figure(figsize=(6,4))\n",
    "    df[media_cols].apply(lambda col: col.fillna(\"no\").astype(str).str.lower().map(lambda x: 1 if x in ['yes','true','1','y'] else 0)).sum().plot(kind='bar')\n",
    "    plt.title(\"Count of media types present\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"media_counts.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ---------------------------\n",
    "# PHASE 4: Text EDA\n",
    "# ---------------------------\n",
    "# Choose English-translated columns when available for NLP parts\n",
    "text_cols_candidates = [\"Eng_Trans_Statement\",\"Eng_Trans_News_Body\",\"Statement\",\"News Body\"]\n",
    "text_cols = [c for c in text_cols_candidates if c in df.columns]\n",
    "\n",
    "print(\"\\nText columns found:\", text_cols)\n",
    "\n",
    "# Basic length stats per text column\n",
    "length_stats = {}\n",
    "for c in text_cols:\n",
    "    length_stats[c] = text_length_stats(df[c])\n",
    "length_df = pd.DataFrame(length_stats).T\n",
    "print(\"\\nText length statistics:\\n\", length_df)\n",
    "length_df.to_csv(\"text_length_stats.csv\")\n",
    "\n",
    "# Compare length by Label (if Label exists)\n",
    "if \"Label\" in df.columns:\n",
    "    for c in text_cols:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        df_nonnull = df[[c,\"Label\"]].dropna(subset=[c])\n",
    "        # create a words column\n",
    "        df_nonnull[\"word_count\"] = df_nonnull[c].astype(str).map(lambda x: len(x.split()))\n",
    "        sns.boxplot(x=\"Label\", y=\"word_count\", data=df_nonnull)\n",
    "        plt.title(f\"Word count by Label for {c}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"wordcount_by_label_{c}.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "# Top words and wordclouds\n",
    "stop_eng = set(stopwords.words('english'))\n",
    "for c in text_cols:\n",
    "    # Top words\n",
    "    top_words = top_n_words(df[c].astype(str).fillna(\"\"), n=50, language='english')\n",
    "    print(f\"\\nTop words for {c}:\\n\", top_words[:25])\n",
    "    # WordCloud\n",
    "    wc = WordCloud(width=800, height=400, collocations=False,\n",
    "                   stopwords=stop_eng).generate(\" \".join(df[c].dropna().astype(str).tolist()))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"WordCloud: {c}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"wordcloud_{c}.png\", dpi=150)\n",
    "    plt.close()\n",
    "    # Top bigrams/trigrams\n",
    "    print(\"Top bigrams:\", top_ngrams(df[c].astype(str).fillna(\"\"), ngram=2, top_n=20)[:10])\n",
    "    print(\"Top trigrams:\", top_ngrams(df[c].astype(str).fillna(\"\"), ngram=3, top_n=10)[:8])\n",
    "\n",
    "# ---------------------------\n",
    "# PHASE 5: Sentiment analysis (on English-translated fields)\n",
    "# ---------------------------\n",
    "# We'll use Eng_Trans_News_Body if present else Eng_Trans_Statement\n",
    "sent_col = None\n",
    "for c in [\"Eng_Trans_News_Body\",\"Eng_Trans_Statement\",\"News Body\",\"Statement\"]:\n",
    "    if c in df.columns:\n",
    "        sent_col = c\n",
    "        break\n",
    "\n",
    "if sent_col:\n",
    "    print(f\"\\nPerforming simple sentiment analysis on {sent_col} using TextBlob (polarity/subjectivity).\")\n",
    "    df[[f\"{sent_col}_polarity\", f\"{sent_col}_subjectivity\"]] = df[sent_col].fillna(\"\").astype(str).apply(simple_sentiment)\n",
    "    # Save sentiment summary\n",
    "    print(df[[f\"{sent_col}_polarity\", f\"{sent_col}_subjectivity\"]].describe())\n",
    "    df[[f\"{sent_col}_polarity\", f\"{sent_col}_subjectivity\"]].to_csv(\"sentiment_summary.csv\")\n",
    "    # Compare polarity by label if available\n",
    "    if \"Label\" in df.columns:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.boxplot(x=\"Label\", y=f\"{sent_col}_polarity\", data=df.dropna(subset=[f\"{sent_col}_polarity\",\"Label\"]))\n",
    "        plt.title(\"Polarity by Label\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"polarity_by_label.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "# ---------------------------\n",
    "# PHASE 6: Cross-analysis & correlations\n",
    "# ---------------------------\n",
    "# Example: Label vs News_Category\n",
    "if \"Label\" in df.columns and \"News_Category\" in df.columns:\n",
    "    ct = pd.crosstab(df[\"News_Category\"], df[\"Label\"], normalize='index')\n",
    "    ct.to_csv(\"category_label_crosstab.csv\")\n",
    "    print(\"\\nCategory vs Label crosstab (percent by category):\\n\", ct.head())\n",
    "\n",
    "# Correlation between numeric-ish features: word counts, sentiment, etc.\n",
    "numeric_feats = []\n",
    "if sent_col:\n",
    "    numeric_feats += [f\"{sent_col}_polarity\", f\"{sent_col}_subjectivity\"]\n",
    "for c in text_cols:\n",
    "    if c in df.columns:\n",
    "        df[f\"{c}_wordcount\"] = df[c].fillna(\"\").astype(str).map(lambda x: len(x.split()))\n",
    "        numeric_feats.append(f\"{c}_wordcount\")\n",
    "\n",
    "# Add binary media indicators\n",
    "for m in media_cols:\n",
    "    df[f\"{m}_flag\"] = df[m].fillna(\"no\").astype(str).str.lower().map(lambda x: 1 if x in ['yes','true','1','y'] else 0)\n",
    "    numeric_feats.append(f\"{m}_flag\")\n",
    "\n",
    "# Label as numeric\n",
    "if \"Label\" in df.columns:\n",
    "    try:\n",
    "        df[\"label_numeric\"] = pd.to_numeric(df[\"Label\"], errors='coerce')\n",
    "        numeric_feats.append(\"label_numeric\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if numeric_feats:\n",
    "    corr = df[numeric_feats].corr()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"vlag\")\n",
    "    plt.title(\"Correlation heatmap (numeric features)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"numeric_correlation_heatmap.png\", dpi=150)\n",
    "    plt.close()\n",
    "    corr.to_csv(\"numeric_feature_correlation.csv\")\n",
    "    print(\"\\nSaved correlation matrix.\")\n",
    "\n",
    "# ---------------------------\n",
    "# PHASE 7: Save cleaned summary outputs\n",
    "# ---------------------------\n",
    "# Basic cleaned snapshot (first N rows) and a CSV for EDA results\n",
    "snapshot = df.head(500)\n",
    "snapshot.to_csv(\"eda_snapshot_first500.csv\", index=False)\n",
    "print(\"\\nSaved eda_snapshot_first500.csv\")\n",
    "\n",
    "# Summaries: missing, duplicates, top categories\n",
    "summary = {\n",
    "    \"shape\": df.shape,\n",
    "    \"missing_fraction\": miss.to_dict(),\n",
    "    \"duplicates_rowcount\": int(dup_count),\n",
    "}\n",
    "# Save summary as JSON-like CSV\n",
    "pd.Series(summary).to_csv(\"eda_summary_overview.csv\")\n",
    "print(\"Saved eda_summary_overview.csv\")\n",
    "\n",
    "print(\"\\nEDA complete. Figures saved as .png and summaries saved as CSVs in current working directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
